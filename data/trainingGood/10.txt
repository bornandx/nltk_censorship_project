b'\n\n\n\n\n \n \n \n \n sklearn.svm.LinearSVC — scikit-learn 0.18.1 documentation\n \n \n \n \n\n \n \n \n \n \n \n \n var DOCUMENTATION_OPTIONS = { \n URL_ROOT : \'../../\ ' , \n VERSION : \'0.18.1\ ' , \n COLLAPSE_INDEX : false , \n FILE_SUFFIX : \'.html\ ' , \n HAS_SOURCE : true\n } ; \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n \n $ ( `` div.buttonNext , div.buttonPrevious '' ) .hover ( \n function ( ) { \n $ ( this ) .css ( \'background-color\ ' , \ ' # FF9C34\ ' ) ; \n } , \n function ( ) { \n $ ( this ) .css ( \'background-color\ ' , \ ' # A7D6E2\ ' ) ; \n } \n ) ; \n function showMenu ( ) { \n var topNav = document.getElementById ( `` scikit-navbar '' ) ; \n if ( topNav.className === `` navbar '' ) { \n topNav.className += `` responsive '' ; \n } else { \n topNav.className = `` navbar '' ; \n } \n } ; \n \n\n \n \n\n\n \n \n \n \n \n \n Home\n Installation\n \n Documentation\n \n \n \n \n Scikit-learn 0.18 ( stable ) \n Tutorials\n User guide\n API\n FAQ\n Contributing\n \n Scikit-learn 0.19-dev ( development ) \n Scikit-learn 0.17\n Scikit-learn 0.16\n Scikit-learn 0.15\n\t\t\t\tPDF documentation\n \n \n \n Examples\n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n\n \n\n\n\n \n \n \n \n \n Previous\n \n \n sklearn.svm.SVC\n \n \n sklearn.svm.SVC\n \n \n \n \n \n \n \n Next\n \n \n sklearn.svm.N ... \n \n \n sklearn.svm.NuSVC\n \n \n \n\n \n \n \n \n \n \n Up\n \n \n API Reference\n \n \n API Reference\n \n \n \n \n \n \n This documentation is for scikit-learn version 0.18.1 — Other versions\n If you use the software , please consider citing scikit-learn.\n \nsklearn.svm.LinearSVC\nExamples using sklearn.svm.LinearSVC\n\n\n\n\n \n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \nsklearn.svm.LinearSVC\xc2\xb6\n\n\nclass sklearn.svm.LinearSVC ( penalty=\'l2\ ' , loss=\'squared_hinge\ ' , dual=True , tol=0.0001 , C=1.0 , multi_class=\'ovr\ ' , fit_intercept=True , intercept_scaling=1 , class_weight=None , verbose=0 , random_state=None , max_iter=1000 ) [ source ] \xc2\xb6\nLinear Support Vector Classification.\nSimilar to SVC with parameter kernel=’linear’ , but implemented in terms of\nliblinear rather than libsvm , so it has more flexibility in the choice of\npenalties and loss functions and should scale better to large numbers of\nsamples.\nThis class supports both dense and sparse input and the multiclass support\nis handled according to a one-vs-the-rest scheme.\nRead more in the User Guide.\n\n\n\n\nParameters : C : float , optional ( default=1.0 ) \n\nPenalty parameter C of the error term.\n\nloss : string , ‘hinge’ or ‘squared_hinge’ ( default=’squared_hinge’ ) \n\nSpecifies the loss function . ‘hinge’ is the standard SVM loss\n ( used e.g . by the SVC class ) while ‘squared_hinge’ is the\nsquare of the hinge loss.\n\npenalty : string , ‘l1’ or ‘l2’ ( default=’l2’ ) \n\nSpecifies the norm used in the penalization . The ‘l2’\npenalty is the standard used in SVC . The ‘l1’ leads to coef_\nvectors that are sparse.\n\ndual : bool , ( default=True ) \n\nSelect the algorithm to either solve the dual or primal\noptimization problem . Prefer dual=False when n_samples > n_features.\n\ntol : float , optional ( default=1e-4 ) \n\nTolerance for stopping criteria.\n\nmulti_class : string , ‘ovr’ or ‘crammer_singer’ ( default=’ovr’ ) : \n\nDetermines the multi-class strategy if y contains more than\ntwo classes.\n '' ovr '' trains n_classes one-vs-rest classifiers , while `` crammer_singer '' \noptimizes a joint objective over all classes.\nWhile crammer_singer is interesting from a theoretical perspective\nas it is consistent , it is seldom used in practice as it rarely leads\nto better accuracy and is more expensive to compute.\nIf `` crammer_singer '' is chosen , the options loss , penalty and dual will\nbe ignored.\n\nfit_intercept : boolean , optional ( default=True ) \n\nWhether to calculate the intercept for this model . If set\nto false , no intercept will be used in calculations\n ( i.e . data is expected to be already centered ) .\n\nintercept_scaling : float , optional ( default=1 ) \n\nWhen self.fit_intercept is True , instance vector x becomes\n [ x , self.intercept_scaling ] , \ni.e . a “synthetic” feature with constant value equals to\nintercept_scaling is appended to the instance vector.\nThe intercept becomes intercept_scaling * synthetic feature weight\nNote ! the synthetic feature weight is subject to l1/l2 regularization\nas all other features.\nTo lessen the effect of regularization on synthetic feature weight\n ( and therefore on the intercept ) intercept_scaling has to be increased.\n\nclass_weight : { dict , ‘balanced’ } , optional\n\nSet the parameter C of class i to class_weight [ i ] *C for\nSVC . If not given , all classes are supposed to have\nweight one.\nThe “balanced” mode uses the values of y to automatically adjust\nweights inversely proportional to class frequencies in the input data\nas n_samples / ( n_classes * np.bincount ( y ) ) \n\nverbose : int , ( default=0 ) \n\nEnable verbose output . Note that this setting takes advantage of a\nper-process runtime setting in liblinear that , if enabled , may not work\nproperly in a multithreaded context.\n\nrandom_state : int seed , RandomState instance , or None ( default=None ) \n\nThe seed of the pseudo random number generator to use when\nshuffling the data.\n\nmax_iter : int , ( default=1000 ) \n\nThe maximum number of iterations to be run.\n\n\n\nAttributes : coef_ : array , shape = [ n_features ] if n_classes == 2 else [ n_classes , n_features ] \n\nWeights assigned to the features ( coefficients in the primal\nproblem ) . This is only available in the case of a linear kernel.\ncoef_ is a readonly property derived from raw_coef_ that\nfollows the internal memory layout of liblinear.\n\nintercept_ : array , shape = [ 1 ] if n_classes == 2 else [ n_classes ] \n\nConstants in decision function.\n\n\n\n\n\n\nSee also\n\nSVC\nImplementation of Support Vector Machine classifier using libsvm : the kernel can be non-linear but its SMO algorithm does not scale to large number of samples as LinearSVC does . Furthermore SVC multi-class mode is implemented using one vs one scheme while LinearSVC uses one vs the rest . It is possible to implement one vs the rest with SVC by using the sklearn.multiclass.OneVsRestClassifier wrapper . Finally SVC can fit dense data without memory copy if the input is C-contiguous . Sparse data will still incur memory copy though.\nsklearn.linear_model.SGDClassifier\nSGDClassifier can optimize the same cost function as LinearSVC by adjusting the penalty and loss parameters . In addition it requires less memory , allows incremental ( online ) learning , and implements various loss functions and regularization regimes.\n\n\nNotes\nThe underlying C implementation uses a random number generator to\nselect features when fitting the model . It is thus not uncommon\nto have slightly different results for the same input data . If\nthat happens , try with a smaller tol parameter.\nThe underlying implementation , liblinear , uses a sparse internal\nrepresentation for the data that will incur a memory copy.\nPredict output may not match that of standalone liblinear in certain\ncases . See differences from liblinear\nin the narrative documentation.\nReferences\nLIBLINEAR : A Library for Large Linear Classification\nMethods\n\n\n\n\n\n\ndecision_function ( X ) \nPredict confidence scores for samples.\n\ndensify ( ) \nConvert coefficient matrix to dense array format.\n\nfit ( X , y [ , sample_weight ] ) \nFit the model according to the given training data.\n\nfit_transform ( X [ , y ] ) \nFit to data , then transform it.\n\nget_params ( [ deep ] ) \nGet parameters for this estimator.\n\npredict ( X ) \nPredict class labels for samples in X.\n\nscore ( X , y [ , sample_weight ] ) \nReturns the mean accuracy on the given test data and labels.\n\nset_params ( \\*\\*params ) \nSet the parameters of this estimator.\n\nsparsify ( ) \nConvert coefficient matrix to sparse format.\n\ntransform ( \\*args , \\*\\*kwargs ) \nDEPRECATED : Support to use estimators as feature selectors will be removed in version 0.19.\n\n\n\n\n\n__init__ ( penalty=\'l2\ ' , loss=\'squared_hinge\ ' , dual=True , tol=0.0001 , C=1.0 , multi_class=\'ovr\ ' , fit_intercept=True , intercept_scaling=1 , class_weight=None , verbose=0 , random_state=None , max_iter=1000 ) [ source ] \xc2\xb6\n\n\n\n\ndecision_function ( X ) [ source ] \xc2\xb6\nPredict confidence scores for samples.\nThe confidence score for a sample is the signed distance of that\nsample to the hyperplane.\n\n\n\n\nParameters : X : { array-like , sparse matrix } , shape = ( n_samples , n_features ) \n\nSamples.\n\n\n\nReturns : array , shape= ( n_samples , ) if n_classes == 2 else ( n_samples , n_classes ) : \n\nConfidence scores per ( sample , class ) combination . In the binary\ncase , confidence score for self.classes_ [ 1 ] where > 0 means this\nclass would be predicted.\n\n\n\n\n\n\n\n\n\ndensify ( ) [ source ] \xc2\xb6\nConvert coefficient matrix to dense array format.\nConverts the coef_ member ( back ) to a numpy.ndarray . This is the\ndefault format of coef_ and is required for fitting , so calling\nthis method is only required on models that have previously been\nsparsified ; otherwise , it is a no-op.\n\n\n\n\nReturns : self : estimator : \n\n\n\n\n\n\n\nfit ( X , y , sample_weight=None ) [ source ] \xc2\xb6\nFit the model according to the given training data.\n\n\n\n\nParameters : X : { array-like , sparse matrix } , shape = [ n_samples , n_features ] \n\nTraining vector , where n_samples in the number of samples and\nn_features is the number of features.\n\ny : array-like , shape = [ n_samples ] \n\nTarget vector relative to X\n\nsample_weight : array-like , shape = [ n_samples ] , optional\n\nArray of weights that are assigned to individual\nsamples . If not provided , \nthen each sample is given unit weight.\n\n\n\nReturns : self : object\n\nReturns self.\n\n\n\n\n\n\n\n\n\nfit_transform ( X , y=None , **fit_params ) [ source ] \xc2\xb6\nFit to data , then transform it.\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n\n\n\n\nParameters : X : numpy array of shape [ n_samples , n_features ] \n\nTraining set.\n\ny : numpy array of shape [ n_samples ] \n\nTarget values.\n\n\n\nReturns : X_new : numpy array of shape [ n_samples , n_features_new ] \n\nTransformed array.\n\n\n\n\n\n\n\n\n\nget_params ( deep=True ) [ source ] \xc2\xb6\nGet parameters for this estimator.\n\n\n\n\nParameters : deep : boolean , optional\n\nIf True , will return the parameters for this estimator and\ncontained subobjects that are estimators.\n\n\n\nReturns : params : mapping of string to any\n\nParameter names mapped to their values.\n\n\n\n\n\n\n\n\n\npredict ( X ) [ source ] \xc2\xb6\nPredict class labels for samples in X.\n\n\n\n\nParameters : X : { array-like , sparse matrix } , shape = [ n_samples , n_features ] \n\nSamples.\n\n\n\nReturns : C : array , shape = [ n_samples ] \n\nPredicted class label per sample.\n\n\n\n\n\n\n\n\n\nscore ( X , y , sample_weight=None ) [ source ] \xc2\xb6\nReturns the mean accuracy on the given test data and labels.\nIn multi-label classification , this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n\n\n\n\nParameters : X : array-like , shape = ( n_samples , n_features ) \n\nTest samples.\n\ny : array-like , shape = ( n_samples ) or ( n_samples , n_outputs ) \n\nTrue labels for X.\n\nsample_weight : array-like , shape = [ n_samples ] , optional\n\nSample weights.\n\n\n\nReturns : score : float\n\nMean accuracy of self.predict ( X ) wrt . y.\n\n\n\n\n\n\n\n\n\nset_params ( **params ) [ source ] \xc2\xb6\nSet the parameters of this estimator.\nThe method works on simple estimators as well as on nested objects\n ( such as pipelines ) . The latter have parameters of the form\n < component > __ < parameter > so that it’s possible to update each\ncomponent of a nested object.\n\n\n\n\nReturns : self : \n\n\n\n\n\n\n\nsparsify ( ) [ source ] \xc2\xb6\nConvert coefficient matrix to sparse format.\nConverts the coef_ member to a scipy.sparse matrix , which for\nL1-regularized models can be much more memory- and storage-efficient\nthan the usual numpy.ndarray representation.\nThe intercept_ member is not converted.\n\n\n\n\nReturns : self : estimator : \n\n\n\nNotes\nFor non-sparse models , i.e . when there are not many zeros in coef_ , \nthis may actually increase memory usage , so use this method with\ncare . A rule of thumb is that the number of zero elements , which can\nbe computed with ( coef_ == 0 ) .sum ( ) , must be more than 50 % for this\nto provide significant benefits.\nAfter calling this method , further fitting with the partial_fit\nmethod ( if any ) will not work until you call densify.\n\n\n\n\ntransform ( *args , **kwargs ) [ source ] \xc2\xb6\nDEPRECATED : Support to use estimators as feature selectors will be removed in version 0.19 . Use SelectFromModel instead.\nReduce X to its most important features.\n\nUses coef_ or feature_importances_ to determine the most\nimportant features . For models with a coef_ for each class , the\nabsolute sum over the classes is used.\n\n\n\n\nParameters : X : array or scipy sparse matrix of shape [ n_samples , n_features ] \n\n\nThe input samples.\n\n\nthreshold\n : string , float or None , optional ( default=None ) The threshold value to use for feature selection . Features whose\nimportance is greater or equal are kept while the others are\ndiscarded . If “median” ( resp . “mean” ) , then the threshold value is\nthe median ( resp . the mean ) of the feature importances . A scaling\nfactor ( e.g . , “1.25*mean” ) may also be used . If None and if\navailable , the object attribute threshold is used . Otherwise , \n“mean” is used by default.\n\n\n\n\n\nReturns : X_r : array of shape [ n_samples , n_selected_features ] \n\nThe input samples with only the selected features.\n\n\n\n\n\n\n\n\n\n\nExamples using sklearn.svm.LinearSVC\xc2\xb6\n\n\nSelecting dimensionality reduction with Pipeline and GridSearchCV\n\n\n\nExplicit feature map approximation for RBF kernels\n\n\n\nProbability Calibration curves\n\n\n\nComparison of Calibration of Classifiers\n\n\n\nPlot different SVM classifiers in the iris dataset\n\n\n\nScaling the regularization parameter for SVCs\n\n\n\nClassification of text documents using sparse features\n\n\n\n\n\n \n \n \n \n \n \n\n \n © 2010 - 2016 , scikit-learn developers ( BSD License ) .\n Show this page source\n \n \n \n \n Previous\n \n \n \n Next\n \n \n \n \n\n \n \n var _gaq = _gaq || [ ] ; \n _gaq.push ( [ \'_setAccount\ ' , \'UA-22606712-2\ ' ] ) ; \n _gaq.push ( [ \'_trackPageview\ ' ] ) ; \n\n ( function ( ) { \n var ga = document.createElement ( \'script\ ' ) ; ga.type = \'text/javascript\ ' ; ga.async = true ; \n ga.src = ( \'https : \ ' == document.location.protocol ? \'https : //ssl\ ' : \'http : //www\ ' ) + \'.google-analytics.com/ga.js\ ' ; \n var s = document.getElementsByTagName ( \'script\ ' ) [ 0 ] ; s.parentNode.insertBefore ( ga , s ) ; \n } ) ( ) ; \n \n \n\n \n google.load ( \'search\ ' , \'1\ ' , \n { language : \'en\ ' } ) ; google.setOnLoadCallback ( function ( ) { \n var customSearchControl = new\n google.search.CustomSearchControl ( \'016639176250731907682 : tjtqbvtvij0\ ' ) ; \n customSearchControl.setResultSetSize ( google.search.Search.FILTERED_CSE_RESULTSET ) ; \n var options = new google.search.DrawOptions ( ) ; \n options.setAutoComplete ( true ) ; \n customSearchControl.draw ( \'cse\ ' , options ) ; } , true ) ; \n \n \n ' 